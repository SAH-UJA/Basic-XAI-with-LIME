{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XAI with LIME.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOLsj6ZF23vuUxOphl28eJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9de0b788d9614f4da717c1645b8cea11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f6e441656872481283f0d89ca90b8f21",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_04a6913f1ef142f2bfedb069d5d1b5ee",
              "IPY_MODEL_9840b3bc46aa4beaa2b4441c5477182a"
            ]
          }
        },
        "f6e441656872481283f0d89ca90b8f21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "04a6913f1ef142f2bfedb069d5d1b5ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_05b921fc519d4de2bc088c00595d9754",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2a006a10cdee45d8b19ab8f3f5399922"
          }
        },
        "9840b3bc46aa4beaa2b4441c5477182a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cd0a13391d524a949eadd7b9c7d2cc92",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000/1000 [00:04&lt;00:00, 226.31it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_541a61efd20e40949c3f1ced79334062"
          }
        },
        "05b921fc519d4de2bc088c00595d9754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2a006a10cdee45d8b19ab8f3f5399922": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd0a13391d524a949eadd7b9c7d2cc92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "541a61efd20e40949c3f1ced79334062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAH-UJA/Basic-XAI-with-LIME/blob/main/XAI_with_LIME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yl_fpuFHKDv"
      },
      "source": [
        "# Explainable AI with Python's LIME library\r\n",
        "In this blog we'll be using LIME to get an explanation as to why a particular prediction is being made. We will train a basic MNIST handwritten digit classification model using Keras and then use LIME."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chUMh9oDIUxV"
      },
      "source": [
        "### What is LIME?\r\n",
        "LIME, or Local Interpretable Model-Agnostic Explanations, is an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. It modifies a single data sample by tweaking the feature values and observes the resulting impact on the output. It performs the role of an \"explainer\" to explain predictions from each data sample. The output of LIME is a set of explanations representing the contribution of each feature to a prediction for a single sample, which is a form of local interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpzF4VyxJZPT"
      },
      "source": [
        "### Building a Digit Classifier\r\n",
        "Install tensorflow using the following command on command prompt. Make sure you have **pip** set as environment variable.\r\n",
        "```\r\n",
        "pip install tensorflow\r\n",
        "```\r\n",
        "Tensorflow is one the most widely used frameworks for deep learning. Keras has now been included in the tensorflow distribution therefore you need not install it separately. Keras frontend helps reduce lower level training complexity and it a good place to quickly build models. In this case, we have Tensorflow backend with Keras frontend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIWSHtdqPguI"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHj2gPN7MAqd"
      },
      "source": [
        "Keras has mnist dataset as a part of its distribution which can be loaded by using *load_data()* method of mnist module. This method returns two tuples representing training and testing data arranged for supervised learning and so we have used x and y representing image and target label respectively. \r\n",
        "\r\n",
        "Now, the image returned is a 1-D numpy vector of size 784 that is typsecasted from int to float32 and converted to a 2-D matrix of size 28x28. Since the image is grayscale its pixel values range from 0 to 255 therefore we normalize is by dividing by 255.0. This step is an important preprocessing step and large numbers increase training complexity and therefore it is recommended to normalize them between 0 to 1.    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07VroclGPrVQ"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "x_train = x_train.reshape((-1,28,28,1)).astype('float32') / 255.0\r\n",
        "x_test = x_test.reshape((-1,28,28,1)).astype('float32') / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX7pi1O3NwkM"
      },
      "source": [
        "Since the lime module that we are using works only with 3-D images, i.e. an image having 3 channels generally RGB, we replicate the grayscale plane here. This code segment converts a grayscale image to RGB by just replicating the available plane. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46aAnDNxTRFk"
      },
      "source": [
        "import numpy as np\r\n",
        "def to_rgb(x):\r\n",
        "    x_rgb = np.zeros((x.shape[0], 28, 28, 3))\r\n",
        "    for i in range(3):\r\n",
        "        x_rgb[..., i] = x[..., 0]\r\n",
        "    return x_rgb\r\n",
        "x_train = to_rgb(x_train)\r\n",
        "x_test = to_rgb(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkEtb7pSOfGs"
      },
      "source": [
        "Sequential API of Keras helps us to quickly create a model but the only drawback is that it is not flexible, i.e. we can have one input and one output unlike the functional API of Keras where we can have multiple inputs and multiple outputs.\r\n",
        "\r\n",
        "This model takes a 3-D image and passes it to a *Conv2D* layers having 16 filters each of size 3x3 and having an activation function of ReLU. Convolution layers basically learns the filter weights and bias and intuitively acts as \"eyes\" for the model and return feature maps which are then passed to *MaxPooling2D* which by default has a max filter of size 2x2 and it solely reduces the dimension of the feature map from convolutional layer and preserves region of interest to some extent. \r\n",
        "\r\n",
        "Finally, we flatten the feature map and add a dense layer that should give us a vector of size 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg0bK6LHP-RU"
      },
      "source": [
        "model = keras.Sequential(\r\n",
        "    [\r\n",
        "     keras.Input(shape=(28,28,3)),\r\n",
        "     layers.Conv2D(16, 3, activation='relu'),\r\n",
        "     layers.MaxPooling2D(),\r\n",
        "     layers.Flatten(),\r\n",
        "     layers.Dense(10)\r\n",
        "    ]\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZcGHIgdQe7z"
      },
      "source": [
        "We use *SparseCategoricalCrossentropy(from_logits=True)* since we got a vector of size 10 and we wish to apply a softmax on that to get a one-hot encoded vector. Here, Adam optimizer is used to train the model based on the error in the actual and predicted one-hot vectors. These vector should have one only for the label to which the image belongs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4cmDVUKQhAm"
      },
      "source": [
        "model.compile(\r\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
        "    optimizer=keras.optimizers.Adam(),\r\n",
        "    metrics=['accuracy']\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV4Eh1XZRR5j"
      },
      "source": [
        "The basic CNN model is trained using *model.fit()* for 2 epochs with a batch_size of 32 and a validation set is fed which was segregated earlier while loading the mnist data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAHjsDq-Qr8t",
        "outputId": "7531ffc9-6f4d-4cd0-d39d-b21bc2fbbb61"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=2, batch_size=32, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 22s 12ms/step - loss: 0.4055 - accuracy: 0.8816 - val_loss: 0.0895 - val_accuracy: 0.9731\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0912 - accuracy: 0.9726 - val_loss: 0.0768 - val_accuracy: 0.9748\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7575378438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvUhaWXnRvp6"
      },
      "source": [
        "Our model is trained with a training accuracy of 97% and validation accuracy of 97% which is good enough for this demo. Now, we install LIME module of python using **pip**.\r\n",
        "\r\n",
        "```\r\n",
        "pip install lime\r\n",
        "```\r\n",
        "You can also install modules on jupyter notebook or google colab notebooks using an ! (exclamation symbol) as shown.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0dvI8TyQ2wY",
        "outputId": "01a9dfc3-903f-4b21-bb37-dcb57ae124c1"
      },
      "source": [
        "!pip install lime\r\n",
        "\r\n",
        "import lime\r\n",
        "from lime import lime_image\r\n",
        "from skimage.segmentation import mark_boundaries\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lime in /usr/local/lib/python3.6/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from lime) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from lime) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lime) (1.19.4)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.6/dist-packages (from lime) (0.16.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lime) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from lime) (4.41.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->lime) (1.0.0)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (7.0.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->lime) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN6VUeRpSq-9"
      },
      "source": [
        "*y_train* stores the actual label of each image in the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taFyrlKeU9BD",
        "outputId": "60443318-78b6-43f0-c324-762c9fc4b65b"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqyoR5METafN"
      },
      "source": [
        "Now, we find *y_pred_train* which stores the prediction of the model on training image data, *x_train*. The prediction per image is a 1-D vector of 10 numbers and we extract the label by finding the index position of the number with max value. Now, we compare the predicted and actual labels and count the number of bad predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GmkFg_aRGDu",
        "outputId": "be9e123d-3542-49fe-e84d-ab7b145231ec"
      },
      "source": [
        "y_pred_train = model.predict(x_train)\r\n",
        "bad_predictions = (y_pred_train.argmax(axis=1) != y_train)\r\n",
        "print('Bad predictions:', sum(bad_predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bad predictions: 1189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHAKeQD-UoRJ"
      },
      "source": [
        "Our model is ready and it is time to use LIME for XAI. Using the *lime_image* module of lime package we create an explainer of LimeImageExplainer() class. This object has a method *explain_instance()* which takes 3-d image data and a predictor function, here, model.predict, and based on the predictions by the model's predictor function it returns an explanation. The explanation object has a method *get_image_and_mask()* which takes predicted labels corresponding the 3-d image data earlier parsed and return as (image, mask) tuple, where image is a 3d numpy array and mask is a 2d numpy array that can be used with *skimage.segmentation.mark_boundaries*. The returned image with the corresponding mask represents the features in the image responsible for the prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334,
          "referenced_widgets": [
            "9de0b788d9614f4da717c1645b8cea11",
            "f6e441656872481283f0d89ca90b8f21",
            "04a6913f1ef142f2bfedb069d5d1b5ee",
            "9840b3bc46aa4beaa2b4441c5477182a",
            "05b921fc519d4de2bc088c00595d9754",
            "2a006a10cdee45d8b19ab8f3f5399922",
            "cd0a13391d524a949eadd7b9c7d2cc92",
            "541a61efd20e40949c3f1ced79334062"
          ]
        },
        "id": "pmmfdc_2RR4V",
        "outputId": "7ecfc3b9-ef1d-47aa-dc4c-2ba99e91b2d3"
      },
      "source": [
        "explainer = lime_image.LimeImageExplainer(random_state=42)\r\n",
        "explanation = explainer.explain_instance(x_train[10], model.predict)\r\n",
        "plt.imshow(x_train[10])\r\n",
        "temp, mask = explanation.get_image_and_mask(model.predict(x_train[10].reshape((1,28,28,3))).argmax(axis=1)[0], positive_only=True, hide_rest=False)\r\n",
        "plt.imshow(mark_boundaries(temp, mask))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9de0b788d9614f4da717c1645b8cea11",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f757255bd30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBSJZC0eYRq9"
      },
      "source": [
        "With this we come to an end of this blog, I hope it was comprehensive enough. For more info on lime, please refer the [docs](https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_image). Enjoy and don't forget to like this article if you find this useful and follow me for updates on my future articles.  "
      ]
    }
  ]
}